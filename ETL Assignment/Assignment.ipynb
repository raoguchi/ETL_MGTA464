{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-connector-python\n",
      "  Using cached snowflake_connector_python-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (42.0.8)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (24.2.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2024.7.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.12.2)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.15.4)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.2.2)\n",
      "Collecting tomlkit (from snowflake-connector-python)\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0->snowflake-connector-python) (1.26.19)\n",
      "Using cached snowflake_connector_python-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: asn1crypto, tomlkit, snowflake-connector-python\n",
      "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.12.1 tomlkit-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user='roguchi',\n",
    "    password='Rsm123123',\n",
    "    account='nchcfow-qtb66258'\n",
    ")\n",
    "cs = conn.cursor()\n",
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS maindb\") # Creating Database, Stage for PO CSV Files, and Use Public Schema\n",
    "cs.execute('USE DATABASE maindb')\n",
    "cs.execute('CREATE OR REPLACE STAGE POStage')\n",
    "cs.execute('USE SCHEMA PUBLIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "file_path = Path('Data/Monthly_PO_Data') # Iterates Through Monthly_PO_Data Folder to PUT CSV files for staging\n",
    "for fp in file_path.iterdir():\n",
    "    if fp.is_file():\n",
    "        cs.execute(f\"PUT file://{fp} @POStage\")\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\n",
    "    \"CREATE OR REPLACE TABLE PO_Data(\"\n",
    "    \"PurchaseOrderID STRING, \"\n",
    "    \"SupplierID STRING, \"\n",
    "    \"OrderDate date, \"\n",
    "    \"PurchaseOrderLineID integer,\"\n",
    "    \"StockItemID integer, \"\n",
    "    \"ReceivedOuters integer, \"\n",
    "    \"ExpectedUnitPricePerOuter float, \"\n",
    "    \"LastReceiptDate date, \"\n",
    "    \"IsOrderLineFinalized boolean)\"\n",
    ") # Create PO_Data Table to house all CSV PO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PO_Data (PurchaseOrderID, SupplierID, OrderDate, PurchaseOrderLineID, StockItemID, ReceivedOuters, ExpectedUnitPricePerOuter, LastReceiptDate, IsOrderLineFinalized)\n",
    "FROM (SELECT $1, $2, $3, $13, $14, $17, $19, $20, $21 FROM @POStage)\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV',\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',\n",
    "    TIMESTAMP_FORMAT = 'MM/DD/YYYY HH24:MI'\n",
    "    SKIP_HEADER = 1\n",
    ")\n",
    "ON_ERROR = 'CONTINUE'\n",
    "\"\"\") # Select only relevant columns from the PO Data, While Copying in Data From Staged CSV Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"ALTER TABLE PO_Data ADD COLUMN POAmount FLOAT;\") # Create new column POAmount From calcuations\n",
    "cs.execute(\"UPDATE PO_Data SET POAmount = ReceivedOuters * ExpectedUnitPricePerOuter\") # Update PO_Data Table with said calcuations\n",
    "cs.execute(\"\"\"CREATE OR REPLACE TABLE PO_Data AS\n",
    "           SELECT PURCHASEORDERID, SupplierID, OrderDate, sum(POAMOUNT) as POAMOUNT FROM PO_DATA\n",
    "           GROUP BY PURCHASEORDERID, SUPPLIERID, ORDERDATE;\n",
    "           \"\"\") ### Perform an Aggregation ON POAmount to do sum per line item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE OR REPLACE STAGE xml_stage\") # Create stage for XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_path = Path(\"Supplier_Transactions_XML.xml\")\n",
    "cs.execute(f\"PUT file://Data/{xml_path} @xml_stage\") # PUT XML file into new stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"CREATE OR REPLACE TABLE xml_staging_table (\n",
    "    raw_data VARIANT\n",
    ");\"\"\") # Create Variant Table where XML file contents are stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"COPY INTO xml_staging_table\n",
    "        FROM @xml_stage\n",
    "        FILE_FORMAT = (TYPE = 'XML')\n",
    "\"\"\") # Copy said XML data into the table that was created in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"CREATE OR REPLACE TABLE supplier_transactions (\n",
    "    supplier_transaction_id STRING,\n",
    "    supplier_id STRING,\n",
    "    purchase_order_id STRING,\n",
    "    supplier_invoice_number STRING,\n",
    "    transaction_date STRING,\n",
    "    amount_excluding_tax float,\n",
    "    tax_amount float,\n",
    "    transaction_amount float,\n",
    "    outstanding_balance float,\n",
    "    finalization_date STRING,\n",
    "    is_finalized INTEGER\n",
    ");\"\"\") # Create a table where Shreded XML data will be housed with relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"INSERT INTO supplier_transactions\n",
    "SELECT\n",
    "    XMLGET(flattened.value, 'SupplierTransactionID'):\"$\"::STRING as SupplierTransactionID,\n",
    "    XMLGET(flattened.value, 'SupplierID'):\"$\"::STRING as SupplierID,\n",
    "    XMLGET(flattened.value, 'PurchaseOrderID'):\"$\"::STRING as PurchaseOrderID,\n",
    "    XMLGET(flattened.value, 'SupplierInvoiceNumber'):\"$\"::STRING as SupplierInvoiceNumber,\n",
    "    XMLGET(flattened.value, 'TransactionDate'):\"$\"::STRING as TransactionDate,\n",
    "    XMLGET(flattened.value, 'AmountExcludingTax'):\"$\"::FLOAT as AmountExcludingTax,\n",
    "    XMLGET(flattened.value, 'TaxAmount'):\"$\"::FLOAT as TaxAmount,\n",
    "    XMLGET(flattened.value, 'TransactionAmount'):\"$\"::FLOAT as TransactionAmount,\n",
    "    XMLGET(flattened.value, 'OutstandingBalance'):\"$\"::FLOAT as OutstandingBalance,\n",
    "    XMLGET(flattened.value, 'FinalizationDate'):\"$\"::STRING as FinalizationDate,\n",
    "    XMLGET(flattened.value, 'IsFinalized'):\"$\"::INTEGER as IsFinalized,\n",
    "FROM \n",
    "    XML_STAGING_TABLE,\n",
    "    LATERAL FLATTEN(input => xml_staging_table.raw_data:\"$\") AS flattened\n",
    "WHERE \n",
    "    GET(flattened.value, '@') = 'row';\n",
    "\n",
    "\"\"\") # Shred XML data and house into the table that was created in the cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE Purchase_Orders_And_Invoices_Table AS (\n",
    "SELECT *, AMOUNT_EXCLUDING_TAX - POAMOUNT AS Invoiced_vs_Quoted FROM SUPPLIER_TRANSACTIONS AS A\n",
    "JOIN PO_DATA AS B\n",
    "ON B.PURCHASEORDERID = A.PURCHASE_ORDER_ID);\n",
    "\"\"\") # Joined Shreded XML table with PO_Data Table on PurchaseOrderID and create new calculated field under POAmount\n",
    "#This creates 0 across the two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"Create OR REPLACE Materialized VIEW Purchase_Orders_And_Invoices AS (SELECT * FROM Purchase_Orders_And_Invoices_Table)\")\n",
    "# Create materialized view from new table above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def execute_pgsql_file(file_path):\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rsm-docker\",\n",
    "        user=\"jovyan\",\n",
    "        password=\"postgres\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=\"8765\"\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Open and execute the .pgsql file\n",
    "    with open(file_path, 'r') as f:\n",
    "        cursor.execute(f.read())\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Example usage\n",
    "pgsql_fp = Path('Data/supplier_case.pgsql')\n",
    "execute_pgsql_file(pgsql_fp) #Parse through PGSQL File to read contents of said File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_csv():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rsm-docker\",\n",
    "        user=\"jovyan\",\n",
    "        password=\"postgres\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=\"8765\"\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    export_query = \"COPY (SELECT * FROM supplier_case) TO STDOUT WITH CSV HEADER\"\n",
    "    \n",
    "    with open('Data/supplier_case.csv', 'w') as f:\n",
    "        cursor.copy_expert(export_query, f)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "export_data_to_csv()\n",
    "#COPY Table created from PGSQL Script into a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute('CREATE OR REPLACE STAGE Supplier_Case')\n",
    "cs.execute('USE SCHEMA public')\n",
    "# Create a stage for newly created CSV File from the PGSQL Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_snowflake(csv_file_path):\n",
    "    conn = snowflake.connector.connect(\n",
    "        user='roguchi',\n",
    "        password='Rsm123123',\n",
    "        account='nchcfow-qtb66258',\n",
    "        database='maindb',\n",
    "        schema='public'\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    put_command = f\"PUT file://{csv_file_path} @Supplier_Case\"\n",
    "    cursor.execute(put_command)\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "csv_fp = Path('Data/supplier_case.csv')\n",
    "upload_to_snowflake(csv_fp)\n",
    "\n",
    "# Put CSV file for staging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_load_table_in_snowflake(create_table_statement):\n",
    "    conn = snowflake.connector.connect(\n",
    "        user='roguchi',\n",
    "        password='Rsm123123',\n",
    "        account='nchcfow-qtb66258',\n",
    "        database='maindb',\n",
    "        schema='Public'\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(create_table_statement)\n",
    "    \n",
    "    copy_command = \"\"\"COPY INTO supplier_case (SupplierID, SupplierName, PostalCityID, Phonenumber, Deliveryaddressline1, DeliveryAddressLine2,\n",
    "    DeliveryPostalCode, DeliveryLocation, PostalAddressLine1, PostalAddressLine2, PostalPostalCode)\n",
    "                      FROM (SELECT $1, $2, $7, $16, $19, $20, $21, $22, $23, $24, $25 FROM @Supplier_case)\n",
    "                        FILE_FORMAT=(TYPE=CSV,\n",
    "                                    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',\n",
    "                                    TIMESTAMP_FORMAT = 'MM/DD/YYYY HH24:MI'\n",
    "                                    SKIP_HEADER = 1\n",
    "                                    )\n",
    "                        ON_ERROR = 'CONTINUE'\"\"\"\n",
    "    # Copy the CSV file contents with relevant columns into Supplier_case table\n",
    "    cursor.execute(copy_command)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "create_table = (\"\"\"\n",
    "                CREATE OR REPLACE TABLE Supplier_Case (\n",
    "                supplierid INTEGER,\n",
    "                suppliername STRING,\n",
    "                postalcityid INTEGER,\n",
    "                phonenumber STRING,\n",
    "                deliveryaddressline1 STRING,\n",
    "                deliveryaddressline2 STRING,\n",
    "                DeliveryPostalCode INTEGER,\n",
    "                deliverylocation STRING,\n",
    "                postaladdressline1 STRING,\n",
    "                postaladdressline2 STRING,\n",
    "                postalpostalcode INTEGER\n",
    "                )\n",
    "                \"\"\")\n",
    "    # SQL Script for creating table where CSV contents are later copied into\n",
    "\n",
    "\n",
    "create_and_load_table_in_snowflake(create_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE OR REPLACE STAGE txtstage\") # Make Stage for TXT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_path = Path(\"2021_Gaz_zcta_national.txt\")\n",
    "cs.execute(f\"PUT file://Data/{txt_path} @txtstage\") # Put TXT File for staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"CREATE OR REPLACE TABLE zcta_geolocation (\n",
    "    GEOID VARCHAR(5),\n",
    "    INTPTLAT FLOAT,\n",
    "    INTPTLONG FLOAT\n",
    ");\n",
    "\"\"\") # Create table for TXT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "COPY INTO zcta_geolocation (GEOID, INTPTLAT, INTPTLONG)\n",
    "FROM (SELECT $1, $6, $7 FROM @txtstage)\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV',\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',\n",
    "    FIELD_DELIMITER = '\\t',\n",
    "    SKIP_HEADER = 1\n",
    ")\n",
    "\"\"\") # Copy in contents for TXT file into the newly created table from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE supplier_zip_geolocation AS (\n",
    "        SELECT sc.postalpostalcode, z.INTPTLAT AS latitude, z.INTPTLONG AS longitude\n",
    "        FROM supplier_case sc\n",
    "        JOIN zcta_geolocation z ON sc.postalpostalcode = z.GEOID\n",
    "        GROUP BY sc.postalpostalcode, z.INTPTLAT, z.INTPTLONG);\n",
    "\"\"\") # Tie each supplier address to a zip code to have matching Longitude & Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE zip_weather_station_mapping AS\n",
    "    SELECT \n",
    "        sz.postalpostalcode,\n",
    "        ws.noaa_weather_station_id AS station_id,\n",
    "        ws.noaa_weather_station_name AS station_name,\n",
    "        st_distance(\n",
    "            st_makepoint(sz.longitude, sz.latitude),\n",
    "            st_makepoint(ws.longitude, ws.latitude)\n",
    "        ) AS distance\n",
    "    FROM maindb.public.supplier_zip_geolocation sz\n",
    "    JOIN WEATHER__ENVIRONMENT.cybersyn.noaa_weather_station_index ws\n",
    "    ON st_distance(st_makepoint(sz.longitude, sz.latitude), st_makepoint(ws.longitude, ws.latitude)) < 50000\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY sz.postalpostalcode ORDER BY distance ASC) = 1;\n",
    "\"\"\") # Calculate qualified zip codes that are close enough to a weather station by using Latitude/Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE zip_code_weather_data AS\n",
    "           SELECT \n",
    "            z.postalpostalcode,\n",
    "            wm.date,\n",
    "            wm.value as Max_Temp,\n",
    "            wm.variable\n",
    "            FROM maindb.public.zip_weather_station_mapping z\n",
    "            JOIN WEATHER__ENVIRONMENT.cybersyn.noaa_weather_metrics_timeseries wm\n",
    "            ON z.station_id = wm.noaa_weather_station_id\n",
    "            WHERE wm.variable = 'maximum_temperature';\n",
    "\"\"\") # Tie time series maximum temperature weather readings to qualified weather_stations/zipcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE supplier_zip_code_weather_table AS \n",
    "SELECT A.PostalPostalCode, B.date, B.Max_temp FROM supplier_case as A\n",
    "JOIN zip_code_weather_data as B ON A.postalpostalcode = B.postalpostalcode\n",
    "ORDER BY DATE;\n",
    "\"\"\") # Join the Supplier Zip Codes with the time series data created in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE OR REPLACE MATERIALIZED VIEW supplier_zip_code_weather AS (SELECT * FROM supplier_zip_code_weather_table);\")\n",
    "# Create materialized view from table above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f0ad0d8b6d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"CREATE OR REPLACE VIEW order_zip_temp AS \n",
    "SELECT A.*, C.SupplierName, C.PostalCityID, C.Phonenumber, C.DELIVERYADDRESSLINE2, C.DeliveryPostalCode, C.DeliveryLocation, C.PostalAddressLine1\n",
    "FROM supplier_zip_code_weather AS A\n",
    "LEFT JOIN purchase_orders_and_invoices AS B ON A.DATE = B.transaction_date\n",
    "LEFT JOIN supplier_case AS C ON A.postalpostalcode = C.postalpostalcode\n",
    "WHERE A.max_temp is NOT NULL;\"\"\")\n",
    "# Tie all supplier information, transaction information, time series weather data into 1 table by joining on zipcode and dates\n",
    "# This returns only 1 zipcode as this is the only one with 'high temperature' readings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
